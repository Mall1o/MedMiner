{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKlY_7J-tsIr",
        "outputId": "77f581f9-5079-4066-f8a8-86fdb70306dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch_geometric.data import HeteroData\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Carichiamo il dataset\n",
        "file_path = '/content/filtered_data_con_etichetta.csv'\n",
        "dataset = pd.read_csv(file_path)\n",
        "\n",
        "# 1. Creiamo le mappature uniche di pazienti e malattie\n",
        "paziente_diagnosi_malattia = dataset[['CODICE_FISCALE_ASSISTITO', 'ICD9_CM']]\n",
        "unique_pazienti = paziente_diagnosi_malattia['CODICE_FISCALE_ASSISTITO'].unique()\n",
        "unique_malattie = paziente_diagnosi_malattia['ICD9_CM'].unique()\n",
        "\n",
        "# Mappature per indici\n",
        "paziente_id_mapping = {code: idx for idx, code in enumerate(unique_pazienti)}\n",
        "malattia_id_mapping = {code: idx for idx, code in enumerate(unique_malattie)}\n",
        "\n",
        "# Convertiamo in indici\n",
        "pazienti_indices = paziente_diagnosi_malattia['CODICE_FISCALE_ASSISTITO'].map(paziente_id_mapping).to_numpy()\n",
        "malattie_indices = paziente_diagnosi_malattia['ICD9_CM'].map(malattia_id_mapping).to_numpy()\n",
        "\n",
        "# 2. Creiamo il grafo eterogeneo\n",
        "data = HeteroData()\n",
        "\n",
        "# Aggiungiamo i nodi\n",
        "data['paziente'].num_nodes = len(unique_pazienti)\n",
        "data['malattia'].num_nodes = len(unique_malattie)\n",
        "\n",
        "# Aggiungiamo gli edge index per la relazione paziente-malattia (diagnosi)\n",
        "data['paziente', 'diagnosi', 'malattia'].edge_index = torch.tensor([pazienti_indices, malattie_indices], dtype=torch.long)\n",
        "data['malattia', 'diagnosi', 'paziente'].edge_index = torch.tensor([malattie_indices, pazienti_indices], dtype=torch.long)\n",
        "\n",
        "scaler_pazienti = StandardScaler()\n",
        "scaler_malattie = StandardScaler()\n",
        "\n",
        "\n",
        "# 3. Preprocessare le feature dei pazienti\n",
        "def process_pazienti_features(df):\n",
        "    df['SESSO'] = df['SESSO'].map({'M': 0, 'F': 1})  # Codifica del sesso\n",
        "    df['SESSO'] = df['SESSO'].fillna(df['SESSO'].mode()[0])  # Riempimento dei NaN in 'SESSO' con la moda\n",
        "    df['DATA_PRESCRIZIONE'] = pd.to_datetime(df['DATA_PRESCRIZIONE'])\n",
        "    df['ETÀ'] = df['DATA_PRESCRIZIONE'].dt.year - df['ANNO_NASCITA']  # Calcolo dell'età\n",
        "    df['ETÀ'] = df['ETÀ'].fillna(df['ETÀ'].median())  # Riempimento dei NaN\n",
        "    pazienti_features = scaler_pazienti.fit_transform(df[['SESSO', 'ETÀ']])  # Normalizzazione\n",
        "    return pazienti_features\n",
        "\n",
        "# Preprocessare le feature delle malattie\n",
        "def process_malattie_features(df):\n",
        "    df['ICD9_CM_CODE'] = df['ICD9_CM'].astype('category').cat.codes  # Codifica ICD9\n",
        "    df['ETICHETTA_CODE'] = df['etichetta'].astype('category').cat.codes  # Codifica etichetta\n",
        "    df['ICD9_CM_CODE'] = df['ICD9_CM_CODE'].fillna(df['ICD9_CM_CODE'].median())  # Riempimento dei NaN\n",
        "    df['ETICHETTA_CODE'] = df['ETICHETTA_CODE'].fillna(df['ETICHETTA_CODE'].median())  # Riempimento dei NaN\n",
        "    malattie_features = scaler_malattie.fit_transform(df[['ICD9_CM_CODE', 'ETICHETTA_CODE']])  # Normalizzazione\n",
        "    return malattie_features\n",
        "\n",
        "# 4. Preprocessiamo e aggiungiamo le feature dei pazienti e delle malattie al grafo\n",
        "# Raggruppiamo i pazienti per CODICE_FISCALE_ASSISTITO\n",
        "pazienti_unique = dataset.groupby('CODICE_FISCALE_ASSISTITO').agg({\n",
        "    'SESSO': 'first',\n",
        "    'ANNO_NASCITA': 'first',\n",
        "    'DATA_PRESCRIZIONE': 'max'\n",
        "}).reset_index()\n",
        "pazienti_unique['idx'] = pazienti_unique['CODICE_FISCALE_ASSISTITO'].map(paziente_id_mapping)\n",
        "pazienti_unique = pazienti_unique.sort_values('idx')\n",
        "\n",
        "# Feature dei pazienti\n",
        "pazienti_features = process_pazienti_features(pazienti_unique)\n",
        "data['paziente'].x = torch.tensor(pazienti_features, dtype=torch.float)\n",
        "\n",
        "# Preprocessiamo le feature delle malattie\n",
        "malattie_unique = dataset[['ICD9_CM', 'etichetta']].drop_duplicates()\n",
        "malattie_unique['idx'] = malattie_unique['ICD9_CM'].map(malattia_id_mapping)\n",
        "malattie_unique = malattie_unique.sort_values('idx')\n",
        "\n",
        "# Feature delle malattie\n",
        "malattie_features = process_malattie_features(malattie_unique)\n",
        "data['malattia'].x = torch.tensor(malattie_features, dtype=torch.float)\n",
        "\n",
        "# Rimozione nodi paziente con NaN\n",
        "valid_pazienti_indices = ~torch.isnan(data['paziente'].x).any(dim=1)\n",
        "valid_pazienti_ids = torch.arange(data['paziente'].num_nodes)[valid_pazienti_indices]\n",
        "data['paziente'].x = data['paziente'].x[valid_pazienti_indices]\n",
        "\n",
        "# Aggiorna gli edge_index per i pazienti\n",
        "mask_paziente_edges = torch.isin(data['paziente', 'diagnosi', 'malattia'].edge_index[0], valid_pazienti_ids)\n",
        "data['paziente', 'diagnosi', 'malattia'].edge_index = data['paziente', 'diagnosi', 'malattia'].edge_index[:, mask_paziente_edges]\n",
        "\n",
        "# Rimozione nodi malattia con NaN\n",
        "valid_malattie_indices = ~torch.isnan(data['malattia'].x).any(dim=1)\n",
        "valid_malattie_ids = torch.arange(data['malattia'].num_nodes)[valid_malattie_indices]\n",
        "data['malattia'].x = data['malattia'].x[valid_malattie_indices]\n",
        "\n",
        "# Aggiorna gli edge_index per le malattie\n",
        "mask_malattia_edges = torch.isin(data['paziente', 'diagnosi', 'malattia'].edge_index[1], valid_malattie_ids)\n",
        "data['paziente', 'diagnosi', 'malattia'].edge_index = data['paziente', 'diagnosi', 'malattia'].edge_index[:, mask_malattia_edges]\n",
        "\n",
        "# Aggiorna il numero di nodi\n",
        "data['paziente'].num_nodes = data['paziente'].x.size(0)\n",
        "data['malattia'].num_nodes = data['malattia'].x.size(0)\n",
        "\n",
        "# Verifiche finali\n",
        "print(f\"Numero di nodi pazienti: {data['paziente'].num_nodes}, Feature shape: {data['paziente'].x.shape}\")\n",
        "print(f\"Numero di nodi malattie: {data['malattia'].num_nodes}, Feature shape: {data['malattia'].x.shape}\")\n",
        "print(f\"Max indice paziente: {pazienti_indices.max()}, Numero di nodi paziente: {data['paziente'].num_nodes}\")\n",
        "print(f\"Max indice malattia: {malattie_indices.max()}, Numero di nodi malattia: {data['malattia'].num_nodes}\")\n"
      ],
      "metadata": {
        "id": "fUw3ijAcvNRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8336a05a-a010-4a66-ff9c-f3b069124d8c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-f143e4c4e5c7>:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  data['paziente', 'diagnosi', 'malattia'].edge_index = torch.tensor([pazienti_indices, malattie_indices], dtype=torch.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero di nodi pazienti: 1244, Feature shape: torch.Size([1244, 2])\n",
            "Numero di nodi malattie: 2114, Feature shape: torch.Size([2114, 2])\n",
            "Max indice paziente: 1243, Numero di nodi paziente: 1244\n",
            "Max indice malattia: 2113, Numero di nodi malattia: 2114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRvQIGjGrdCG",
        "outputId": "2eaa24d3-d8a1-468d-8daa-512ff8f2638d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HeteroData(\n",
            "  paziente={\n",
            "    num_nodes=1244,\n",
            "    x=[1244, 2],\n",
            "  },\n",
            "  malattia={\n",
            "    num_nodes=2114,\n",
            "    x=[2114, 2],\n",
            "  },\n",
            "  (paziente, diagnosi, malattia)={ edge_index=[2, 999927] },\n",
            "  (malattia, diagnosi, paziente)={ edge_index=[2, 999927] }\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "# Funzione per dividere gli archi in training, test e validation set\n",
        "def train_test_split_edges(data, test_ratio=0.15, val_ratio=0.15):\n",
        "    # Prendiamo gli edge_index per la relazione paziente-malattia\n",
        "    edge_index = data['paziente', 'diagnosi', 'malattia'].edge_index\n",
        "\n",
        "    # Numero totale di archi\n",
        "    num_edges = edge_index.size(1)\n",
        "\n",
        "    # Definiamo quanti archi mettere in test e validation\n",
        "    num_test = int(num_edges * test_ratio)\n",
        "    num_val = int(num_edges * val_ratio)\n",
        "\n",
        "    # Shuffling casuale degli indici degli archi\n",
        "    perm = torch.randperm(num_edges)\n",
        "    edge_index = edge_index[:, perm]\n",
        "\n",
        "    # Dividiamo gli archi\n",
        "    test_edges = edge_index[:, :num_test]\n",
        "    val_edges = edge_index[:, num_test:num_test + num_val]\n",
        "    train_edges = edge_index[:, num_test + num_val:]\n",
        "\n",
        "    return train_edges, val_edges, test_edges\n"
      ],
      "metadata": {
        "id": "m3dKSmBOgf0Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funzione per generare archi negativi (coppie di nodi non collegate)\n",
        "def generate_negative_edges(data, num_neg_edges, edge_index):\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=edge_index,\n",
        "        num_nodes=(data['paziente'].num_nodes, data['malattia'].num_nodes),\n",
        "        num_neg_samples=num_neg_edges,\n",
        "        force_undirected=False  # Questo dipende dalla direzione delle tue relazioni\n",
        "    )\n",
        "    return neg_edge_index\n"
      ],
      "metadata": {
        "id": "jNsPT2iagg9_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eseguiamo il train-test split\n",
        "train_edges, val_edges, test_edges = train_test_split_edges(data)\n",
        "\n",
        "# Numero di archi negativi da generare (pari agli archi positivi in ogni set)\n",
        "num_train_neg = train_edges.size(1)\n",
        "num_val_neg = val_edges.size(1)\n",
        "num_test_neg = test_edges.size(1)\n",
        "\n",
        "# Generiamo archi negativi per training, validation e test set\n",
        "train_neg_edges = generate_negative_edges(data, num_train_neg, train_edges)\n",
        "val_neg_edges = generate_negative_edges(data, num_val_neg, val_edges)\n",
        "test_neg_edges = generate_negative_edges(data, num_test_neg, test_edges)\n",
        "\n",
        "# Costruiamo gli edge_label per il training\n",
        "train_edge_index = torch.cat([train_edges, train_neg_edges], dim=1)\n",
        "train_edge_label = torch.cat([torch.ones(train_edges.size(1)), torch.zeros(train_neg_edges.size(1))])\n",
        "\n",
        "# Costruiamo gli edge_label per il validation\n",
        "val_edge_index = torch.cat([val_edges, val_neg_edges], dim=1)\n",
        "val_edge_label = torch.cat([torch.ones(val_edges.size(1)), torch.zeros(val_neg_edges.size(1))])\n",
        "\n",
        "# Costruiamo gli edge_label per il test\n",
        "test_edge_index = torch.cat([test_edges, test_neg_edges], dim=1)\n",
        "test_edge_label = torch.cat([torch.ones(test_edges.size(1)), torch.zeros(test_neg_edges.size(1))])\n",
        "\n",
        "print(f\"Train edges: {train_edge_index.shape}, Train labels: {train_edge_label.shape}\")\n",
        "print(f\"Validation edges: {val_edge_index.shape}, Validation labels: {val_edge_label.shape}\")\n",
        "print(f\"Test edges: {test_edge_index.shape}, Test labels: {test_edge_label.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loYcO7-3gi1R",
        "outputId": "c3d23697-3cef-45c7-d8e0-c5cbfd2bb6c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train edges: torch.Size([2, 1399898]), Train labels: torch.Size([1399898])\n",
            "Validation edges: torch.Size([2, 299978]), Validation labels: torch.Size([299978])\n",
            "Test edges: torch.Size([2, 299978]), Test labels: torch.Size([299978])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.nn import HeteroConv, SAGEConv, Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Definiamo il modello con Dropout\n",
        "class HeteroGNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels, dropout=0.2):\n",
        "        super(HeteroGNN, self).__init__()\n",
        "\n",
        "        self.dropout = dropout  # Probabilità di dropout\n",
        "\n",
        "        # Primo livello di convoluzione eterogenea\n",
        "        self.conv1 = HeteroConv({\n",
        "            ('paziente', 'diagnosi', 'malattia'): SAGEConv((-1, -1), hidden_channels),\n",
        "            ('malattia', 'diagnosi', 'paziente'): SAGEConv((-1, -1), hidden_channels)\n",
        "        }, aggr='mean')\n",
        "\n",
        "        # Secondo livello di convoluzione eterogenea\n",
        "        self.conv2 = HeteroConv({\n",
        "            ('paziente', 'diagnosi', 'malattia'): SAGEConv((-1, -1), hidden_channels),\n",
        "            ('malattia', 'diagnosi', 'paziente'): SAGEConv((-1, -1), hidden_channels)\n",
        "        }, aggr='mean')\n",
        "\n",
        "        # Terzo livello di convoluzione\n",
        "        self.conv3 = HeteroConv({\n",
        "            ('paziente', 'diagnosi', 'malattia'): SAGEConv((-1, -1), hidden_channels),\n",
        "            ('malattia', 'diagnosi', 'paziente'): SAGEConv((-1, -1), hidden_channels)\n",
        "        }, aggr='mean')\n",
        "\n",
        "        # Layer finale per la predizione dei link\n",
        "        self.decoder = Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        # Primo livello di convoluzione con Dropout\n",
        "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
        "        x_dict = {key: F.dropout(F.relu(x), p=self.dropout, training=self.training) for key, x in x_dict.items()}\n",
        "\n",
        "        # Secondo livello di convoluzione con Dropout\n",
        "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
        "        x_dict = {key: F.dropout(F.relu(x), p=self.dropout, training=self.training) for key, x in x_dict.items()}\n",
        "\n",
        "        # Terzo livello di convoluzione con Dropout\n",
        "        x_dict = self.conv3(x_dict, edge_index_dict)\n",
        "        x_dict = {key: F.dropout(F.relu(x), p=self.dropout, training=self.training) for key, x in x_dict.items()}\n",
        "\n",
        "        return x_dict\n",
        "\n",
        "    def decode(self, z_dict, edge_label_index):\n",
        "        src, dst = edge_label_index\n",
        "        paziente_emb = z_dict['paziente'][src]\n",
        "        malattia_emb = z_dict['malattia'][dst]\n",
        "        return torch.sigmoid((paziente_emb * malattia_emb).sum(dim=-1))"
      ],
      "metadata": {
        "id": "Is0Ap75uvReR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Ipotizziamo che hidden_channels = 64 e out_channels = 1 (per probabilità di link)\n",
        "model = HeteroGNN(hidden_channels=64, out_channels=1)\n",
        "\n",
        "# Funzione di perdita (Binary Cross Entropy per link prediction)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "# Definizione dell'ottimizzatore\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Definizione dello scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n"
      ],
      "metadata": {
        "id": "ZlZ1twDehufP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a83027-2aec-48ce-a35d-3af1fc80aeff"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def train(model, data, train_edge_index, train_edge_label, optimizer, loss_fn):\n",
        "    model.train()  # Modalità training\n",
        "    optimizer.zero_grad()  # Azzeriamo i gradienti\n",
        "\n",
        "    # Forward pass\n",
        "    z_dict = model(data.x_dict, data.edge_index_dict)\n",
        "    pred = model.decode(z_dict, train_edge_index)\n",
        "\n",
        "    # Calcoliamo la perdita\n",
        "    loss = loss_fn(pred, train_edge_label.float())\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calcoliamo l'accuratezza\n",
        "    pred_label = (pred > 0.5).float()\n",
        "    acc = accuracy_score(train_edge_label.cpu(), pred_label.cpu())\n",
        "\n",
        "    return loss.item(), acc\n",
        "\n",
        "def evaluate(model, data, edge_index, edge_label):\n",
        "    model.eval()  # Modalità evaluation\n",
        "    with torch.no_grad():\n",
        "        # Forward pass\n",
        "        z_dict = model(data.x_dict, data.edge_index_dict)\n",
        "        pred = model.decode(z_dict, edge_index)\n",
        "\n",
        "    # Calcoliamo l'accuratezza\n",
        "    pred_label = (pred > 0.5).float()\n",
        "    acc = accuracy_score(edge_label.cpu(), pred_label.cpu())\n",
        "\n",
        "    return acc\n",
        "\n",
        "def test(model, data, edge_index, edge_label):\n",
        "    model.eval()  # Modalità evaluation\n",
        "    with torch.no_grad():\n",
        "        # Forward pass\n",
        "        z_dict = model(data.x_dict, data.edge_index_dict)\n",
        "        pred = model.decode(z_dict, edge_index)\n",
        "\n",
        "    # Calcoliamo varie metriche\n",
        "    pred_label = (pred > 0.5).float()\n",
        "    auc = roc_auc_score(edge_label.cpu(), pred.cpu())\n",
        "    acc = accuracy_score(edge_label.cpu(), pred_label.cpu())\n",
        "    precision = precision_score(edge_label.cpu(), pred_label.cpu())\n",
        "    recall = recall_score(edge_label.cpu(), pred_label.cpu())\n",
        "    f1 = f1_score(edge_label.cpu(), pred_label.cpu())\n",
        "\n",
        "    # Stampiamo tutte le metriche\n",
        "    print(f\"Test AUC: {auc:.4f}\")\n",
        "    print(f\"Test Accuracy: {acc:.4f}\")\n",
        "    print(f\"Test Precision: {precision:.4f}\")\n",
        "    print(f\"Test Recall: {recall:.4f}\")\n",
        "    print(f\"Test F1 Score: {f1:.4f}\")\n",
        "\n",
        "    return auc, acc, precision, recall, f1\n",
        "\n",
        "# Parametri di addestramento\n",
        "num_epochs = 300\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "# Ciclo di addestramento\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # Addestramento su train set\n",
        "    train_loss, train_acc = train(model, data, train_edge_index, train_edge_label, optimizer, loss_fn)\n",
        "\n",
        "    # Aggiorniamo lo scheduler\n",
        "    scheduler.step(train_loss)\n",
        "\n",
        "    # Ogni 10 epoche eseguiamo una valutazione su train e validation set\n",
        "    if epoch % 10 == 0:\n",
        "        val_acc = evaluate(model, data, val_edge_index, val_edge_label)\n",
        "        print(f\"Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "# Dopo l'addestramento, valutiamo il set di test e stampiamo le metriche finali\n",
        "print(\"\\nFinal Test Evaluation:\")\n",
        "test_auc, test_acc, test_precision, test_recall, test_f1 = test(model, data, test_edge_index, test_edge_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZm6vGkUu9yG",
        "outputId": "ff291f28-4a3a-4068-f176-73271db46081"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/300, Train Loss: 0.6945, Train Accuracy: 0.5083, Validation Accuracy: 0.5744\n",
            "Epoch 20/300, Train Loss: 0.6873, Train Accuracy: 0.5370, Validation Accuracy: 0.5572\n",
            "Epoch 30/300, Train Loss: 0.6709, Train Accuracy: 0.5557, Validation Accuracy: 0.6293\n",
            "Epoch 40/300, Train Loss: 0.6640, Train Accuracy: 0.6216, Validation Accuracy: 0.6431\n",
            "Epoch 50/300, Train Loss: 0.6352, Train Accuracy: 0.6423, Validation Accuracy: 0.6430\n",
            "Epoch 60/300, Train Loss: 0.6338, Train Accuracy: 0.6770, Validation Accuracy: 0.7107\n",
            "Epoch 70/300, Train Loss: 0.6017, Train Accuracy: 0.7088, Validation Accuracy: 0.7519\n",
            "Epoch 80/300, Train Loss: 0.6090, Train Accuracy: 0.7128, Validation Accuracy: 0.7654\n",
            "Epoch 90/300, Train Loss: 0.5953, Train Accuracy: 0.7191, Validation Accuracy: 0.7612\n",
            "Epoch 100/300, Train Loss: 0.5748, Train Accuracy: 0.7483, Validation Accuracy: 0.7686\n",
            "Epoch 110/300, Train Loss: 0.5885, Train Accuracy: 0.7459, Validation Accuracy: 0.7796\n",
            "Epoch 120/300, Train Loss: 0.5876, Train Accuracy: 0.7348, Validation Accuracy: 0.7843\n",
            "Epoch 130/300, Train Loss: 0.5773, Train Accuracy: 0.7514, Validation Accuracy: 0.7840\n",
            "Epoch 140/300, Train Loss: 0.5781, Train Accuracy: 0.7526, Validation Accuracy: 0.7888\n",
            "Epoch 150/300, Train Loss: 0.5748, Train Accuracy: 0.7643, Validation Accuracy: 0.7862\n",
            "Epoch 160/300, Train Loss: 0.5805, Train Accuracy: 0.7490, Validation Accuracy: 0.7842\n",
            "Epoch 170/300, Train Loss: 0.5678, Train Accuracy: 0.7577, Validation Accuracy: 0.7826\n",
            "Epoch 180/300, Train Loss: 0.5919, Train Accuracy: 0.7423, Validation Accuracy: 0.7836\n",
            "Epoch 190/300, Train Loss: 0.5773, Train Accuracy: 0.7551, Validation Accuracy: 0.7855\n",
            "Epoch 200/300, Train Loss: 0.5744, Train Accuracy: 0.7632, Validation Accuracy: 0.7858\n",
            "Epoch 210/300, Train Loss: 0.5731, Train Accuracy: 0.7660, Validation Accuracy: 0.7859\n",
            "Epoch 220/300, Train Loss: 0.5741, Train Accuracy: 0.7593, Validation Accuracy: 0.7861\n",
            "Epoch 230/300, Train Loss: 0.5756, Train Accuracy: 0.7660, Validation Accuracy: 0.7859\n",
            "Epoch 240/300, Train Loss: 0.5778, Train Accuracy: 0.7463, Validation Accuracy: 0.7859\n",
            "Epoch 250/300, Train Loss: 0.5699, Train Accuracy: 0.7640, Validation Accuracy: 0.7859\n",
            "Epoch 260/300, Train Loss: 0.5874, Train Accuracy: 0.7442, Validation Accuracy: 0.7859\n",
            "Epoch 270/300, Train Loss: 0.5669, Train Accuracy: 0.7690, Validation Accuracy: 0.7859\n",
            "Epoch 280/300, Train Loss: 0.5665, Train Accuracy: 0.7619, Validation Accuracy: 0.7859\n",
            "Epoch 290/300, Train Loss: 0.5854, Train Accuracy: 0.7501, Validation Accuracy: 0.7859\n",
            "Epoch 300/300, Train Loss: 0.5726, Train Accuracy: 0.7546, Validation Accuracy: 0.7859\n",
            "\n",
            "Final Test Evaluation:\n",
            "Test AUC: 0.8438\n",
            "Test Accuracy: 0.7872\n",
            "Test Precision: 0.7875\n",
            "Test Recall: 0.7867\n",
            "Test F1 Score: 0.7871\n"
          ]
        }
      ]
    }
  ]
}